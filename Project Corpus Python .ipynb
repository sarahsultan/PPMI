{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Final Project: Positive Pointwise Mutual Information</h2>\n",
    "<h2>Corpus Linguistics with Python - Summer Semester 2020</h1>\n",
    "<h3>Sara Sultan - 968430.</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency of the code:\n",
    "In this code PEP8 guidlines are followed, and to make sure of it used pycodestyle module is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sara/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/sara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk import ngrams, word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Corpus Pre-Processing\n",
    "\n",
    "Preprocessing is done mainly by lemmatization and normalization of corpus along with removal of unnecessary characters and adding new line for every sentence. \n",
    "\n",
    "NLTK wordnet lemmatizer is used for lemmatization. Wordnet lemmatizer lemmatizes the words with respect to their POS category, so average_perceptron tagger is used to tag the words in corpus to be lemmatized by wordnet lemmatizer. \n",
    "\n",
    "Firstly, new line is added after every period and question mark, and un-necessary characters are removed .\n",
    "Secondaly, a function 'get_wordnet_tag' is defined to tag words in corpus to be lemmatized by wordnet lemmatizer.\n",
    "Lastly, corpus is split, tagged, and lemmatized in nested for loop, and lemmas are saved in the list 'lemmatized'.\n",
    "\n",
    "\n",
    "Normalization is done before lemmatization because some all capitalized words were not being properly lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_fic.txt', 'r') as text_file:\n",
    "    corpus = text_file.read()\n",
    "\n",
    "corpus = corpus.replace(' . ', ' . \\n')\n",
    "corpus = corpus.replace(' ? ', ' ? \\n')\n",
    "corpus = corpus.replace(' ! ', ' ! \\n')\n",
    "corpus = corpus.replace('. \\n\" <p>', '. \"\\n<p>')\n",
    "corpus = corpus.replace('? \\n\" <p>', '? \"\\n<p>')\n",
    "corpus = corpus.replace('! \\n\" <p>', '! \"\\n<p>')\n",
    "corpus = corpus.replace('<p> ', '')\n",
    "corpus = corpus.replace('``', '\"')\n",
    "\n",
    "# lowercase the corpus before lemmatization\n",
    "corpus = corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89431/89431 [00:48<00:00, 1845.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define function to tag words in corpus\n",
    "def get_wordnet_tag(treebanktag):\n",
    "    if treebanktag[0] == 'J':\n",
    "        return wn.ADJ\n",
    "    elif treebanktag[0] == 'V':\n",
    "        return wn.VERB\n",
    "    elif treebanktag[0] == 'R':\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN\n",
    "\n",
    "\n",
    "# Lemmatization after pos_tagging\n",
    "lemmatized = []\n",
    "for sent in tqdm(corpus.split('\\n')):\n",
    "    for token, tag in pos_tag(sent.split()):\n",
    "        lemma = lemmatizer.lemmatize(token, get_wordnet_tag(tag))\n",
    "        lemmatized.append(lemma)\n",
    "    lemmatized.append('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Preprocessed Data:\n",
    "Preprocessed data is a list which is converted to string, white space after every new line is removed and the resultant text is saved to a text file 'coca.preprocessed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_preprocessed = ' '.join(lemmatized)\n",
    "coca_preprocessed = coca_preprocessed.replace('\\n ', '\\n')\n",
    "with open(\"coca.preprocessed.text\", \"w\") as fp:\n",
    "    fp.write(coca_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Counting Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Extraction of Bigrams and Unigrams\n",
    "This step involves following steps:\n",
    "\n",
    "   1. Uigrams and Bigrams are extracted from the pre-processed data using ngrams and bigrams modules respectively.\n",
    "\n",
    "   2. Frequencies of both Unigrams and Bigrams are calculated by frequency distribution module of NLTK.\n",
    "\n",
    "   3. Unigrams and Bigrams with their respective frequency counts are stored in separate files in dataframe.\n",
    "   \n",
    "   4. Unigrams saved in dataframe in above step, were in tuple. They are being extracted as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7:80: E501 line too long (92 > 79 characters)\n",
      "12:80: E501 line too long (95 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "# Remove \\n from lemmatized tokens\n",
    "lemmatized = [lemma for lemma in lemmatized if lemma != '\\n']\n",
    "\n",
    "# Extract  Bigrams and their frequencies into a dataframe\n",
    "bigrams = nltk.bigrams(lemmatized)\n",
    "freq_dist_bigrams = nltk.FreqDist(bigrams)\n",
    "df_bigrams = pd.DataFrame(freq_dist_bigrams.items(), columns=['Bigrams', 'Frequency_Count'])\n",
    "\n",
    "# Extract  Unigrams and their frequencies into a dataframe\n",
    "unigrams = ngrams(lemmatized, 1)\n",
    "freq_dist_unigrams = nltk.FreqDist(unigrams)\n",
    "df_unigrams = pd.DataFrame(freq_dist_unigrams.items(), columns=['Unigrams', 'Frequency_Count'])\n",
    "df_unigrams['Unigrams'] = df_unigrams['Unigrams'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Changing Frequency to 0 \n",
    "\n",
    "Frequencies of Unigrams and Bigrams having value of 5 or less is turned to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencies less than or equal to 5 are turned to 0\n",
    "df_bigrams.loc[df_bigrams['Frequency_Count'] <= 5, 'Frequency_Count'] = 0\n",
    "df_unigrams.loc[df_unigrams['Frequency_Count'] <= 5, 'Frequency_Count'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Saving Frequency Counts to dictionaries \n",
    "Unigrams and Bigrams with their frequency counts are saved in separate dictionaries, with ngrams as keys and respective frequency counts as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3:80: E501 line too long (82 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "# converting to dictionary\n",
    "bigrams_dict = dict(zip(df_bigrams['Bigrams'], df_bigrams['Frequency_Count']))\n",
    "unigrams_dict = dict(zip(df_unigrams['Unigrams'], df_unigrams['Frequency_Count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: coca.counts file\n",
    "Unigram and Bigram data frames, tab separated, are saved in one csv file 'coca.count' excluding header and indices. File is created with Unigrams in write mode and Bigrams are appended next. This way all Unigrams come first and Bigrams later in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:80: E501 line too long (80 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "df_unigrams.to_csv('coca.counts', sep='\\t', index=False, header=False, mode='w')\n",
    "df_bigrams.to_csv('coca.counts', sep='\\t', index=False, header=False, mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Total number of Tokens\n",
    "Total number of tokens is calculated by summing frequencies of all Unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1347712"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_length = sum(unigrams_dict.values())\n",
    "token_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: PPMI\n",
    "\n",
    "Function is written to calculate PPMI scores. The function takes five arguments and return result as PPMI score as a float number rounded to 3 decimal points.\n",
    "\n",
    "To meet required conditions that function should return 0 if frequency of Unigrams or Bigrams is zero or the words does not exist in the dictionaries of the said ngrams, a collective condition is written which says that if any of the mentioned conditions is met, the function would return 0 as PPMI score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPMI(word1, word2, unigramDict, bigramDict, total_tokens):\n",
    "    # if words are not in dictionaries or frequency of words is zero, return 0\n",
    "    zero_condition = (word1 not in unigramDict) or \\\n",
    "                     (word2 not in unigramDict) or \\\n",
    "                     (unigramDict[word1] == 0) or \\\n",
    "                     (unigramDict[word2] == 0) or \\\n",
    "                     ((word1, word2) not in bigramDict) or \\\n",
    "                     (bigramDict[(word1, word2)] == 0) or \\\n",
    "                     (bigramDict[(word1, word2)] == 0)\n",
    "\n",
    "    if zero_condition:\n",
    "        return 0\n",
    "\n",
    "    # calculate probability of words\n",
    "    prob_word1 = float(unigramDict[word1]) / total_tokens\n",
    "    prob_word2 = float(unigramDict[word2]) / total_tokens\n",
    "    prob_word1_word2 = float(bigramDict[(word1, word2)]) / total_tokens\n",
    "\n",
    "    # calculate PPMI score\n",
    "    PPMI_score = math.log2(prob_word1_word2/(prob_word1*prob_word2))\n",
    "\n",
    "    # Round the PPMI score to 3 decimal points\n",
    "    PPMI_score_round = round(PPMI_score, 3)\n",
    "\n",
    "    return PPMI_score_round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Computing PPMI Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 PPMI Score for all Bigrams\n",
    "\n",
    "PPMI score of Bigrams is computed by applying for loop on Bigrams dictionary, and saved in a list which is later converted to dataframe. The dataframe contains three columns with headers('WORD1', 'WORD2', 'Values'). First and second columns contain individual words of Bigrams and third column named as 'Values' contain their respective PPMI score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4:80: E501 line too long (90 > 79 characters)\n",
      "7:80: E501 line too long (83 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "ppmi_score_list = []\n",
    "for word1, word2 in bigrams_dict:\n",
    "    # Compute the PPMI for all the bigrams in bigrams_dict\n",
    "    ppmi_score_calculation = PPMI(word1, word2, unigrams_dict, bigrams_dict, token_length)\n",
    "    ppmi_score_list.append((word1, word2, ppmi_score_calculation))\n",
    "\n",
    "df_ppmi_score = pd.DataFrame(ppmi_score_list, columns=('WORD1', 'WORD2', 'Values'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Save PPMI Score of Bigrams in a file\n",
    "\n",
    "PPMI score of Bigrams, tab separated, are saved without head and indices in a csv file named 'coca.ppmi'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:80: E501 line too long (80 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "df_ppmi_score.to_csv('coca.ppmi', sep='\\t', index=False, header=False, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Function to print highest 20 PPMI Score \n",
    "\n",
    "A new function 'topN' is defined to sort the Bigrams in descending order of their PPMI score and print top 20 Bigrams with their PPMI scores excluding header and indices. \n",
    "\n",
    "Function takes two arguments i.e data and number of items to be printed, where number of items to be printed is set at the default value of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(df, number_of_items=20):\n",
    "    score_sort = df.sort_values(by='Values', ascending=False)\n",
    "    selected_items = score_sort.head(number_of_items)\n",
    "    header_index_removed = selected_items.to_string(index=False, header=False)\n",
    "    print(header_index_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  guiseppi    scapellini  17.777\n",
      "      sint          holo  17.777\n",
      "      vito         adamo  17.362\n",
      "       tel          aviv  17.192\n",
      "     edith  schermerhorn  17.040\n",
      "      palo          alto  17.040\n",
      "    oswald         truxa  16.947\n",
      "      macy        levitt  16.903\n",
      "    chiang           mai  16.903\n",
      "    cecily       scriber  16.848\n",
      "     irene       lashman  16.777\n",
      "  pheasant      theodora  16.777\n",
      "      amos          holt  16.777\n",
      "       del         norte  16.662\n",
      "       slo            mo  16.555\n",
      " anarchist          no.l  16.555\n",
      "       nil         spaar  16.362\n",
      "    moyshe       rabeynu  16.275\n",
      "   clavius         gulch  16.192\n",
      "      zand       dynasty  16.040\n"
     ]
    }
   ],
   "source": [
    "topN(df_ppmi_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Comparing PPMI and Frequency Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Print the top-20 Bigrams sorted by Frequency Counts\n",
    "\n",
    "Bigrams dictionary (containing bigrams with frequency counts) is converted to dataframe to print top 20 bigrams sorted by their frequency. There is no need to give number of Bigrams to be printed as the default value is set to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6:80: E501 line too long (88 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "# Make a dataframe from bigrams dictionary(bigramsDict)\n",
    "frequency_count_list = []\n",
    "for word1, word2 in bigrams_dict:\n",
    "    frequency_count_list.append((word1, word2, bigrams_dict[word1, word2]))\n",
    "\n",
    "df_freq_count = pd.DataFrame(frequency_count_list, columns=('WORD1', 'WORD2', 'Values'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  @    @  61996\n",
      "  .    \"  18447\n",
      "  ,  and   7206\n",
      "  ,    \"   6486\n",
      "  \"    \"   5136\n",
      "  .    i   5051\n",
      " of  the   4617\n",
      " in  the   4403\n",
      "  ?    \"   4263\n",
      "  \"    i   4184\n",
      "  .   he   4128\n",
      " do  n't   3972\n",
      "  .  the   3938\n",
      "  .    #   3301\n",
      "  ,  but   3139\n",
      " it   be   2924\n",
      "  .  she   2902\n",
      " on  the   2786\n",
      " to  the   2746\n",
      "  ,  the   2591\n"
     ]
    }
   ],
   "source": [
    "topN(df_freq_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Differences between Computing simple Frequency vs. using PPMI Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple frequency is the measure of how frequent a word or pair of words occur in the corpus while PMI quantifies the likelihood of co occurance of two words. Keeping in view the results obtained most frequent bigrams are most insignificant and make no sense of themselves as well while PPMI scores show the words that make unique sense when appear together. Top 20 PPMI scored bigrams are names, the probability of them appearing together is higher than pairs of most frequency counts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
